# U-Net for Cloud Detection in Multispectral Satellite Imagery for BeaverCube2 

## Training U-Net on [38-Cloud Training Set](https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset)
The U-Net architecture is used to train a cloud detection model on the 38-Cloud dataset. The 38-Cloud dataset is a cloud
segmentation dataset that contains 38 Landsat 8 scenes  with corresponding cloud masks. For an 
extension to this dataset please see [95-Cloud](https://github.com/SorourMo/95-Cloud-An-Extension-to-38-Cloud-Dataset).

## Beavercube2 Cameras
The BeaverCube2 satellite has two cameras: a Boson LWIR camera and a Bluefox EO camera. The 
sensor resolution of the Boson camera is 320x256 pixels, and the sensor resolution of the Bluefox
camera is 752x480 pixels. The GSD of the images from the Boson camera is ~200m and the GSD of the 
images from the Bluefox camera is ~100m.

Because the Landsat 8 imagery is at a 30m resolution and LWIR bands are at 100m spatial resolution,
the training data was augmented to induce scale and rotation invariance in the network. 

### Requirements
* Download the 38-Cloudset using the link above or from this [38-Cloud Kaggle Dataset](https://www.kaggle.com/datasets/sorour/38cloud-cloud-segmentation-in-satellite-images).  Then move the 38 Cloud dataset to the base directory of this repository. 
* Clone the official Vitis AI repository from [here](https://github.com/Xilinx/Vitis-AI).
 
 ## Training the U-Net Model
Next, install the required packages by running `pip3 install -r requirements.txt` on the machine 
you are using to train. Run `python3 lwir_download.py train` to download the LWIR 
 band for training images. Also run `python3 lwir_download.py test` to download the LWIR 
Landsat 8 band for testing. Then run `python3 main_train.py` to train the U-Net model on the 38-Cloud dataset.
The path to the dataset folder should be set at `GLOBAL_PATH = 'path to 38-cloud dataset'`. The directory tree for the dataset looks like as following:

├──38-Cloud dataset

│------------├──Cloud-Net_trained_on_38-Cloud_training_patches.h5

│------------├──Training

│------------------├──Train blue<br/>
                      .
                      .
                      .

│------------------├──training_patches_38-cloud.csv

│------------├──Test

│------------------├──Test blue<br/>
                      .
                      .
                      .

│------------------├──test_patches_38-cloud.csv

│------------├──Predictions


The training patches are resized to 192 * 192 before each iteration. Then, four corresponding spectral 
bands are stacked together to create a 192 * 192 * 4 array. A ```.log``` file is generated to keep track of the loss values. The loss function used for training is the soft Jaccard loss. 

## Testing U-Net on [38-Cloud Test Set](https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset)
Run `python main_test.py` for getting the predictions. The predicted cloud masks will be generated in the "Predictions" folder. 
Then, use the [Evaluation over 38-Cloud Dataset section](https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset#evaluation-over-38-cloud-dataset) to get the numerical results and predicted cloud masks for the entire scenes. 

## Conversion to DPU 
See the document titled "Vitis AI Tool Use" in the Beavercube2 documentation in the Flight software 
folder for further information. 
To clone the Vitis AI repository and pull the relevant docker containers, run 
```bash
git clone https://github.com/Xilinx/Vitis-AI
docker pull xilinx/vitis-ai:latest
docker pull xilinx/vitis-ai-cpu:1.4.1.978
```
Then start the latest docker container with 
```bash
./docker_run.sh {Container name}
conda activate vitis-ai-pytorch
```
You may need to reinstall the required pip packages in the docker container. Also ensure to update 
the path to models and datasets in the `pytorch_quantization.py` and `main_train_pt.py` files.

After activating the most recent Vitis AI docker container run `python pytorch_quantization.py calib`
to generate configuration information for the quantized model. Then run `python pytorch_quantization.py test`
to export an XIR representation of the model. 

To compile the model, run the ealier 1.4.1.978 docker container and activate the pytorch conda
environment. Then run `vai_c_xir -x /PATH/TO/quantized.xmodel -a /PATH/TO/arch.json -o /OUTPUTPATH -n netname`
with the appropriate path to the .xmodel XIR representation of the model as well as the `arch.json` file 
specific to target DPU. 

After compilation is complete, a .xmodel file will be generated.

### Get Json File
The 'arch.json' file is an important file required by Vitis AI. It works together with Vitis AI compiler to support model compilation with various DPUCVDX8G configurations. The 'arch.json' will be generated by Vitis during the compilation of DPUCVDX8G TRD, it can be found in '$TRD_HOME/vitis_prj/package_out/sd_card' .

It can also be found in the following path:

`$TRD_HOME/vitis_prj/hw/binary_container_1/link/vivado/vpl/prj/prj.gen/sources_1/bd/vck190*/ip/*_DPUCVDX8G_0/arch.json`

**Note:** The `arch.json` file for the DPUCVDX8G TRD is not included in this repository. The 
fingerprint is either "0x601001036088111" or "0x601001036088131". The `arch.json` file currently 
has the fingerprint "0x601001036088111", but again, this might not be the correct fingerprint for
the DPU generated by the default VCK190-ES1 build described in the "Building the Xilinx DPU" 
document in the Beavercube2 documentation in the Flight software folder. 


## Run the Model on the DPU 
Run the model on the DPU using the Vitis AI runtime libraries. The VART libraries should be built with
the bootable DPU image. Then run `python dpu_runner_unet.py` to load the generated .xmodel file. 
For more information on running the model on the DPU, see the Beavercube2 "Vitis AI Tool Use" document in the Flight
software folder in the Google Drive. 








